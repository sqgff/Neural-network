{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from rbm import RBM\n",
    "from autoencoder_rbm import Autoencoder_RBM\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/64times_overlap.csv')\n",
    "df.drop(\"timestamp\", inplace=True, axis=1)\n",
    "#df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "df = min_max_scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Training set: current loss 1.390221  ||  Validation set: current loss 1.390316\n",
      "Validation:  [2467 2466 2464 2495 2496 2465 2468 2494 2493 2492 2525 2603 2602 2600 2523\n",
      " 2497 2524 2601 2463  906 2522 2448  907 2455  908]\n",
      "Iter 2: Training set: current loss 1.205209  ||  Validation set: current loss 1.205270\n",
      "Validation:  [2467 2466 2495 2496 2464 2465 2468 2494 2493 2492 2525 2603 2602 2600 2523\n",
      " 2497 2524 2601 2463  906 2522  907 2448 2455  908]\n",
      "Iter 3: Training set: current loss 1.042839  ||  Validation set: current loss 1.042945\n",
      "Validation:  [2467 2466 2496 2495 2464 2465 2468 2494 2493 2492 2525 2603 2602 2600 2523\n",
      " 2497 2524 2601 2522 2463  906  907 2448  908 2455]\n",
      "Iter 4: Training set: current loss 0.904687  ||  Validation set: current loss 0.904684\n",
      "Validation:  [2467 2466 2496 2495 2465 2464 2468 2494 2493 2492 2525 2603 2602 2600 2523\n",
      " 2497 2524 2601 2522 2463  906  907 2448  908 2455]\n",
      "Iter 5: Training set: current loss 0.788547  ||  Validation set: current loss 0.788602\n",
      "Validation:  [2467 2496 2466 2495 2465 2464 2468 2494 2493 2492 2525 2603 2602 2600 2523\n",
      " 2497 2524 2601 2522 2463  906  907 2448  908 2526]\n",
      "Iter 6: Training set: current loss 0.696270  ||  Validation set: current loss 0.696306\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2468 2494 2493 2492 2525 2603 2602 2600 2523\n",
      " 2497 2524 2601 2522 2463  906  907 2448  908 2526]\n",
      "Iter 7: Training set: current loss 0.621453  ||  Validation set: current loss 0.621426\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2468 2494 2493 2492 2525 2603 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  906  907 2448 2526  908]\n",
      "Iter 8: Training set: current loss 0.563874  ||  Validation set: current loss 0.563899\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2492 2525 2603 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  906  907 2448 2526  908]\n",
      "Iter 9: Training set: current loss 0.520761  ||  Validation set: current loss 0.520595\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2492 2525 2603 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  906  907 2448 2526  908]\n",
      "Iter 10: Training set: current loss 0.489138  ||  Validation set: current loss 0.489172\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2492 2525 2603 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  906  907 2448 2526  908]\n",
      "Iter 11: Training set: current loss 0.466243  ||  Validation set: current loss 0.466280\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2492 2525 2603 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 12: Training set: current loss 0.448811  ||  Validation set: current loss 0.449061\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2492 2603 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 13: Training set: current loss 0.435487  ||  Validation set: current loss 0.435508\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2492 2603 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 14: Training set: current loss 0.424899  ||  Validation set: current loss 0.425076\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 15: Training set: current loss 0.416164  ||  Validation set: current loss 0.416517\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 16: Training set: current loss 0.409303  ||  Validation set: current loss 0.409363\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 17: Training set: current loss 0.403370  ||  Validation set: current loss 0.403674\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 18: Training set: current loss 0.398301  ||  Validation set: current loss 0.398555\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 19: Training set: current loss 0.393966  ||  Validation set: current loss 0.394091\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 20: Training set: current loss 0.390125  ||  Validation set: current loss 0.390195\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 21: Training set: current loss 0.386307  ||  Validation set: current loss 0.386562\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 22: Training set: current loss 0.383288  ||  Validation set: current loss 0.383401\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 23: Training set: current loss 0.380339  ||  Validation set: current loss 0.380416\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 24: Training set: current loss 0.377516  ||  Validation set: current loss 0.377737\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 25: Training set: current loss 0.375112  ||  Validation set: current loss 0.375109\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2448 2526  908]\n",
      "Iter 26: Training set: current loss 0.372547  ||  Validation set: current loss 0.372670\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 27: Training set: current loss 0.370142  ||  Validation set: current loss 0.370379\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 28: Training set: current loss 0.367875  ||  Validation set: current loss 0.368198\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 29: Training set: current loss 0.365754  ||  Validation set: current loss 0.365900\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2603 2525 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 30: Training set: current loss 0.363372  ||  Validation set: current loss 0.363728\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 31: Training set: current loss 0.361140  ||  Validation set: current loss 0.361145\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2525 2603 2492 2602 2600 2523\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 32: Training set: current loss 0.358569  ||  Validation set: current loss 0.358900\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 33: Training set: current loss 0.355893  ||  Validation set: current loss 0.356204\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 34: Training set: current loss 0.353688  ||  Validation set: current loss 0.353848\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 35: Training set: current loss 0.351923  ||  Validation set: current loss 0.352076\n",
      "Validation:  [2467 2496 2495 2466 2465 2464 2494 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 36: Training set: current loss 0.350064  ||  Validation set: current loss 0.350357\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522 2463  907  906 2526 2448  908]\n",
      "Iter 37: Training set: current loss 0.348345  ||  Validation set: current loss 0.348562\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522  907 2463  906 2526 2448  908]\n",
      "Iter 38: Training set: current loss 0.346606  ||  Validation set: current loss 0.346984\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522  907 2463  906 2526 2448  908]\n",
      "Iter 39: Training set: current loss 0.344760  ||  Validation set: current loss 0.344977\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522  907 2463  906 2526  908 2448]\n",
      "Iter 40: Training set: current loss 0.343102  ||  Validation set: current loss 0.343363\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2600\n",
      " 2524 2497 2601 2522  907 2463  906 2526  908 2448]\n",
      "Iter 41: Training set: current loss 0.341485  ||  Validation set: current loss 0.341575\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907 2463  906 2526  908 2448]\n",
      "Iter 42: Training set: current loss 0.339696  ||  Validation set: current loss 0.339792\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 43: Training set: current loss 0.337815  ||  Validation set: current loss 0.338072\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 44: Training set: current loss 0.336051  ||  Validation set: current loss 0.336220\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 45: Training set: current loss 0.334181  ||  Validation set: current loss 0.334426\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 46: Training set: current loss 0.332362  ||  Validation set: current loss 0.332454\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 47: Training set: current loss 0.330371  ||  Validation set: current loss 0.330676\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 48: Training set: current loss 0.328486  ||  Validation set: current loss 0.328760\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 49: Training set: current loss 0.326556  ||  Validation set: current loss 0.326727\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 50: Training set: current loss 0.324488  ||  Validation set: current loss 0.324712\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 51: Training set: current loss 0.322569  ||  Validation set: current loss 0.322670\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 52: Training set: current loss 0.320617  ||  Validation set: current loss 0.320701\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2448]\n",
      "Iter 53: Training set: current loss 0.318466  ||  Validation set: current loss 0.318558\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908 2469]\n",
      "Iter 54: Training set: current loss 0.316453  ||  Validation set: current loss 0.316480\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2464 2468 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908  909]\n",
      "Iter 55: Training set: current loss 0.314337  ||  Validation set: current loss 0.314231\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908  909]\n",
      "Iter 56: Training set: current loss 0.311810  ||  Validation set: current loss 0.312008\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908  909]\n",
      "Iter 57: Training set: current loss 0.309634  ||  Validation set: current loss 0.309816\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908  909]\n",
      "Iter 58: Training set: current loss 0.307377  ||  Validation set: current loss 0.307740\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908  909]\n",
      "Iter 59: Training set: current loss 0.305061  ||  Validation set: current loss 0.305162\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908  909]\n",
      "Iter 60: Training set: current loss 0.302715  ||  Validation set: current loss 0.303104\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463 2526  908  909]\n",
      "Iter 61: Training set: current loss 0.300350  ||  Validation set: current loss 0.300514\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2523 2524\n",
      " 2600 2497 2601 2522  907  906 2463  908 2526  909]\n",
      "Iter 62: Training set: current loss 0.298068  ||  Validation set: current loss 0.298255\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2497 2601 2522  907  906 2463  908 2526  909]\n",
      "Iter 63: Training set: current loss 0.295615  ||  Validation set: current loss 0.295884\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2497 2601 2522  907  906 2463  908 2526  909]\n",
      "Iter 64: Training set: current loss 0.293193  ||  Validation set: current loss 0.293263\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2497 2601  907 2522  906 2463  908 2526  909]\n",
      "Iter 65: Training set: current loss 0.290664  ||  Validation set: current loss 0.290806\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2497 2601  907 2522  906 2463  908 2526  909]\n",
      "Iter 66: Training set: current loss 0.288053  ||  Validation set: current loss 0.288198\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 67: Training set: current loss 0.285567  ||  Validation set: current loss 0.285642\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 68: Training set: current loss 0.282957  ||  Validation set: current loss 0.283170\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 69: Training set: current loss 0.280328  ||  Validation set: current loss 0.280570\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 70: Training set: current loss 0.277674  ||  Validation set: current loss 0.277922\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 71: Training set: current loss 0.274886  ||  Validation set: current loss 0.275224\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 72: Training set: current loss 0.272234  ||  Validation set: current loss 0.272500\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 73: Training set: current loss 0.269595  ||  Validation set: current loss 0.269759\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 74: Training set: current loss 0.266930  ||  Validation set: current loss 0.267108\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 75: Training set: current loss 0.264037  ||  Validation set: current loss 0.264318\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 76: Training set: current loss 0.261126  ||  Validation set: current loss 0.261452\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 77: Training set: current loss 0.258466  ||  Validation set: current loss 0.258653\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 78: Training set: current loss 0.255539  ||  Validation set: current loss 0.255763\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907  906 2522 2463  908 2526  909]\n",
      "Iter 79: Training set: current loss 0.252958  ||  Validation set: current loss 0.253016\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 80: Training set: current loss 0.250070  ||  Validation set: current loss 0.250302\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 81: Training set: current loss 0.247211  ||  Validation set: current loss 0.247440\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 82: Training set: current loss 0.244452  ||  Validation set: current loss 0.244632\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 83: Training set: current loss 0.241553  ||  Validation set: current loss 0.241671\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 84: Training set: current loss 0.238663  ||  Validation set: current loss 0.238744\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2600 2601 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 85: Training set: current loss 0.235853  ||  Validation set: current loss 0.236081\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 86: Training set: current loss 0.232892  ||  Validation set: current loss 0.233156\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 87: Training set: current loss 0.230015  ||  Validation set: current loss 0.230182\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 88: Training set: current loss 0.227164  ||  Validation set: current loss 0.227287\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 89: Training set: current loss 0.224558  ||  Validation set: current loss 0.224463\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 90: Training set: current loss 0.221559  ||  Validation set: current loss 0.221716\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 91: Training set: current loss 0.218973  ||  Validation set: current loss 0.218871\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 92: Training set: current loss 0.216235  ||  Validation set: current loss 0.216310\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 93: Training set: current loss 0.213555  ||  Validation set: current loss 0.213776\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 94: Training set: current loss 0.210980  ||  Validation set: current loss 0.211080\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526 2463  909]\n",
      "Iter 95: Training set: current loss 0.208452  ||  Validation set: current loss 0.208605\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526 2463  909]\n",
      "Iter 96: Training set: current loss 0.205948  ||  Validation set: current loss 0.206172\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526 2463  909]\n",
      "Iter 97: Training set: current loss 0.203430  ||  Validation set: current loss 0.203678\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526  909 2463]\n",
      "Iter 98: Training set: current loss 0.201055  ||  Validation set: current loss 0.201234\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526  909 2463]\n",
      "Iter 99: Training set: current loss 0.198690  ||  Validation set: current loss 0.199019\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526  909 2463]\n",
      "Iter 100: Training set: current loss 0.196523  ||  Validation set: current loss 0.196842\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 101: Training set: current loss 0.194320  ||  Validation set: current loss 0.194542\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 102: Training set: current loss 0.192255  ||  Validation set: current loss 0.192570\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 103: Training set: current loss 0.190332  ||  Validation set: current loss 0.190550\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 104: Training set: current loss 0.188222  ||  Validation set: current loss 0.188639\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 105: Training set: current loss 0.186278  ||  Validation set: current loss 0.186864\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 106: Training set: current loss 0.184685  ||  Validation set: current loss 0.184918\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 107: Training set: current loss 0.182880  ||  Validation set: current loss 0.183257\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 108: Training set: current loss 0.181178  ||  Validation set: current loss 0.181615\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 109: Training set: current loss 0.179661  ||  Validation set: current loss 0.180092\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 110: Training set: current loss 0.178351  ||  Validation set: current loss 0.178540\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 111: Training set: current loss 0.176924  ||  Validation set: current loss 0.177160\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 112: Training set: current loss 0.175702  ||  Validation set: current loss 0.175906\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 113: Training set: current loss 0.174309  ||  Validation set: current loss 0.174626\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 114: Training set: current loss 0.173163  ||  Validation set: current loss 0.173384\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 115: Training set: current loss 0.172133  ||  Validation set: current loss 0.172364\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 116: Training set: current loss 0.170951  ||  Validation set: current loss 0.171297\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 117: Training set: current loss 0.170050  ||  Validation set: current loss 0.170246\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 118: Training set: current loss 0.169035  ||  Validation set: current loss 0.169351\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 119: Training set: current loss 0.168235  ||  Validation set: current loss 0.168445\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 120: Training set: current loss 0.167408  ||  Validation set: current loss 0.167629\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 121: Training set: current loss 0.166669  ||  Validation set: current loss 0.166909\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 122: Training set: current loss 0.166010  ||  Validation set: current loss 0.166213\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 123: Training set: current loss 0.165358  ||  Validation set: current loss 0.165558\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 124: Training set: current loss 0.164776  ||  Validation set: current loss 0.164976\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 125: Training set: current loss 0.164206  ||  Validation set: current loss 0.164427\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 126: Training set: current loss 0.163678  ||  Validation set: current loss 0.163874\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 127: Training set: current loss 0.163273  ||  Validation set: current loss 0.163402\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 128: Training set: current loss 0.162754  ||  Validation set: current loss 0.162979\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 129: Training set: current loss 0.162398  ||  Validation set: current loss 0.162584\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 130: Training set: current loss 0.161975  ||  Validation set: current loss 0.162167\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 131: Training set: current loss 0.161584  ||  Validation set: current loss 0.161812\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 132: Training set: current loss 0.161310  ||  Validation set: current loss 0.161531\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 133: Training set: current loss 0.160991  ||  Validation set: current loss 0.161254\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 134: Training set: current loss 0.160712  ||  Validation set: current loss 0.161016\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 135: Training set: current loss 0.160563  ||  Validation set: current loss 0.160777\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 136: Training set: current loss 0.160209  ||  Validation set: current loss 0.160512\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 137: Training set: current loss 0.160119  ||  Validation set: current loss 0.160348\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 138: Training set: current loss 0.159922  ||  Validation set: current loss 0.160169\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 139: Training set: current loss 0.159804  ||  Validation set: current loss 0.159975\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 140: Training set: current loss 0.159620  ||  Validation set: current loss 0.159855\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 141: Training set: current loss 0.159544  ||  Validation set: current loss 0.159707\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 142: Training set: current loss 0.159458  ||  Validation set: current loss 0.159581\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 143: Training set: current loss 0.159319  ||  Validation set: current loss 0.159494\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 144: Training set: current loss 0.159196  ||  Validation set: current loss 0.159390\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 145: Training set: current loss 0.159118  ||  Validation set: current loss 0.159317\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 146: Training set: current loss 0.159087  ||  Validation set: current loss 0.159223\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 147: Training set: current loss 0.158994  ||  Validation set: current loss 0.159160\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 148: Training set: current loss 0.158968  ||  Validation set: current loss 0.159083\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 149: Training set: current loss 0.158838  ||  Validation set: current loss 0.159031\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 150: Training set: current loss 0.158754  ||  Validation set: current loss 0.158942\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 151: Training set: current loss 0.158758  ||  Validation set: current loss 0.158923\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 152: Training set: current loss 0.158699  ||  Validation set: current loss 0.158865\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 153: Training set: current loss 0.158667  ||  Validation set: current loss 0.158859\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 154: Training set: current loss 0.158690  ||  Validation set: current loss 0.158788\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 155: Training set: current loss 0.158591  ||  Validation set: current loss 0.158761\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 156: Training set: current loss 0.158619  ||  Validation set: current loss 0.158704\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 157: Training set: current loss 0.158572  ||  Validation set: current loss 0.158683\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 158: Training set: current loss 0.158519  ||  Validation set: current loss 0.158665\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 159: Training set: current loss 0.158462  ||  Validation set: current loss 0.158629\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 160: Training set: current loss 0.158492  ||  Validation set: current loss 0.158598\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 161: Training set: current loss 0.158418  ||  Validation set: current loss 0.158592\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 162: Training set: current loss 0.158440  ||  Validation set: current loss 0.158576\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 163: Training set: current loss 0.158421  ||  Validation set: current loss 0.158545\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 164: Training set: current loss 0.158402  ||  Validation set: current loss 0.158531\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 165: Training set: current loss 0.158364  ||  Validation set: current loss 0.158471\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 166: Training set: current loss 0.158317  ||  Validation set: current loss 0.158485\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 167: Training set: current loss 0.158350  ||  Validation set: current loss 0.158484\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 168: Training set: current loss 0.158322  ||  Validation set: current loss 0.158465\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 169: Training set: current loss 0.158307  ||  Validation set: current loss 0.158413\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 170: Training set: current loss 0.158290  ||  Validation set: current loss 0.158414\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 171: Training set: current loss 0.158293  ||  Validation set: current loss 0.158425\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 172: Training set: current loss 0.158284  ||  Validation set: current loss 0.158424\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 173: Training set: current loss 0.158257  ||  Validation set: current loss 0.158412\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 174: Training set: current loss 0.158274  ||  Validation set: current loss 0.158384\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 175: Training set: current loss 0.158266  ||  Validation set: current loss 0.158398\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 176: Training set: current loss 0.158222  ||  Validation set: current loss 0.158369\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 177: Training set: current loss 0.158278  ||  Validation set: current loss 0.158377\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 178: Training set: current loss 0.158238  ||  Validation set: current loss 0.158360\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 179: Training set: current loss 0.158218  ||  Validation set: current loss 0.158359\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 180: Training set: current loss 0.158200  ||  Validation set: current loss 0.158372\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 181: Training set: current loss 0.158215  ||  Validation set: current loss 0.158345\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 182: Training set: current loss 0.158220  ||  Validation set: current loss 0.158333\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 183: Training set: current loss 0.158228  ||  Validation set: current loss 0.158341\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 184: Training set: current loss 0.158209  ||  Validation set: current loss 0.158302\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 185: Training set: current loss 0.158160  ||  Validation set: current loss 0.158317\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 186: Training set: current loss 0.158229  ||  Validation set: current loss 0.158289\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 187: Training set: current loss 0.158179  ||  Validation set: current loss 0.158315\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 188: Training set: current loss 0.158132  ||  Validation set: current loss 0.158299\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 189: Training set: current loss 0.158123  ||  Validation set: current loss 0.158291\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 190: Training set: current loss 0.158112  ||  Validation set: current loss 0.158307\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 191: Training set: current loss 0.158200  ||  Validation set: current loss 0.158302\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 192: Training set: current loss 0.158110  ||  Validation set: current loss 0.158286\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 193: Training set: current loss 0.158150  ||  Validation set: current loss 0.158269\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 194: Training set: current loss 0.158112  ||  Validation set: current loss 0.158259\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 195: Training set: current loss 0.158147  ||  Validation set: current loss 0.158273\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 196: Training set: current loss 0.158103  ||  Validation set: current loss 0.158250\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 197: Training set: current loss 0.158192  ||  Validation set: current loss 0.158272\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 198: Training set: current loss 0.158096  ||  Validation set: current loss 0.158267\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 199: Training set: current loss 0.158121  ||  Validation set: current loss 0.158230\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 200: Training set: current loss 0.158097  ||  Validation set: current loss 0.158225\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 201: Training set: current loss 0.158115  ||  Validation set: current loss 0.158234\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 202: Training set: current loss 0.158106  ||  Validation set: current loss 0.158247\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 203: Training set: current loss 0.158131  ||  Validation set: current loss 0.158212\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 204: Training set: current loss 0.158066  ||  Validation set: current loss 0.158238\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 205: Training set: current loss 0.158054  ||  Validation set: current loss 0.158204\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 206: Training set: current loss 0.158099  ||  Validation set: current loss 0.158217\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 207: Training set: current loss 0.158050  ||  Validation set: current loss 0.158202\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 208: Training set: current loss 0.158063  ||  Validation set: current loss 0.158196\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 209: Training set: current loss 0.158050  ||  Validation set: current loss 0.158211\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 210: Training set: current loss 0.158031  ||  Validation set: current loss 0.158200\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 211: Training set: current loss 0.157986  ||  Validation set: current loss 0.158195\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 212: Training set: current loss 0.158072  ||  Validation set: current loss 0.158182\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 213: Training set: current loss 0.158031  ||  Validation set: current loss 0.158191\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 214: Training set: current loss 0.157976  ||  Validation set: current loss 0.158179\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 215: Training set: current loss 0.158028  ||  Validation set: current loss 0.158158\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 216: Training set: current loss 0.157998  ||  Validation set: current loss 0.158155\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 217: Training set: current loss 0.158018  ||  Validation set: current loss 0.158157\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 218: Training set: current loss 0.157991  ||  Validation set: current loss 0.158139\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 219: Training set: current loss 0.157966  ||  Validation set: current loss 0.158123\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 220: Training set: current loss 0.157982  ||  Validation set: current loss 0.158134\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 221: Training set: current loss 0.157984  ||  Validation set: current loss 0.158122\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 222: Training set: current loss 0.157973  ||  Validation set: current loss 0.158126\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 223: Training set: current loss 0.157997  ||  Validation set: current loss 0.158123\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 224: Training set: current loss 0.157986  ||  Validation set: current loss 0.158094\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 225: Training set: current loss 0.157943  ||  Validation set: current loss 0.158107\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 226: Training set: current loss 0.157963  ||  Validation set: current loss 0.158094\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 227: Training set: current loss 0.157922  ||  Validation set: current loss 0.158089\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 228: Training set: current loss 0.157915  ||  Validation set: current loss 0.158084\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 229: Training set: current loss 0.157930  ||  Validation set: current loss 0.158052\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 230: Training set: current loss 0.157933  ||  Validation set: current loss 0.158066\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 231: Training set: current loss 0.157901  ||  Validation set: current loss 0.158061\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 232: Training set: current loss 0.157919  ||  Validation set: current loss 0.158048\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 233: Training set: current loss 0.157921  ||  Validation set: current loss 0.158039\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 234: Training set: current loss 0.157908  ||  Validation set: current loss 0.158037\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 235: Training set: current loss 0.157867  ||  Validation set: current loss 0.158029\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 236: Training set: current loss 0.157869  ||  Validation set: current loss 0.158032\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 237: Training set: current loss 0.157918  ||  Validation set: current loss 0.158028\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 238: Training set: current loss 0.157875  ||  Validation set: current loss 0.158002\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 239: Training set: current loss 0.157874  ||  Validation set: current loss 0.158024\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 240: Training set: current loss 0.157870  ||  Validation set: current loss 0.158013\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 241: Training set: current loss 0.157890  ||  Validation set: current loss 0.157996\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 242: Training set: current loss 0.157823  ||  Validation set: current loss 0.157989\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 243: Training set: current loss 0.157849  ||  Validation set: current loss 0.157964\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 244: Training set: current loss 0.157799  ||  Validation set: current loss 0.157987\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 245: Training set: current loss 0.157851  ||  Validation set: current loss 0.157965\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 246: Training set: current loss 0.157823  ||  Validation set: current loss 0.157960\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 247: Training set: current loss 0.157785  ||  Validation set: current loss 0.157955\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 248: Training set: current loss 0.157808  ||  Validation set: current loss 0.157957\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 249: Training set: current loss 0.157796  ||  Validation set: current loss 0.157926\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 250: Training set: current loss 0.157816  ||  Validation set: current loss 0.157939\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 251: Training set: current loss 0.157779  ||  Validation set: current loss 0.157903\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 252: Training set: current loss 0.157751  ||  Validation set: current loss 0.157898\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 253: Training set: current loss 0.157779  ||  Validation set: current loss 0.157888\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 254: Training set: current loss 0.157777  ||  Validation set: current loss 0.157887\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 255: Training set: current loss 0.157695  ||  Validation set: current loss 0.157909\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 256: Training set: current loss 0.157731  ||  Validation set: current loss 0.157884\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 257: Training set: current loss 0.157658  ||  Validation set: current loss 0.157861\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 258: Training set: current loss 0.157694  ||  Validation set: current loss 0.157850\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 259: Training set: current loss 0.157730  ||  Validation set: current loss 0.157843\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 260: Training set: current loss 0.157666  ||  Validation set: current loss 0.157825\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 261: Training set: current loss 0.157717  ||  Validation set: current loss 0.157834\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 262: Training set: current loss 0.157617  ||  Validation set: current loss 0.157832\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 263: Training set: current loss 0.157638  ||  Validation set: current loss 0.157825\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 264: Training set: current loss 0.157627  ||  Validation set: current loss 0.157804\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 265: Training set: current loss 0.157651  ||  Validation set: current loss 0.157817\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 266: Training set: current loss 0.157619  ||  Validation set: current loss 0.157826\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 267: Training set: current loss 0.157646  ||  Validation set: current loss 0.157797\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 268: Training set: current loss 0.157635  ||  Validation set: current loss 0.157796\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 269: Training set: current loss 0.157644  ||  Validation set: current loss 0.157769\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 270: Training set: current loss 0.157637  ||  Validation set: current loss 0.157756\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 271: Training set: current loss 0.157604  ||  Validation set: current loss 0.157746\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 272: Training set: current loss 0.157609  ||  Validation set: current loss 0.157730\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 273: Training set: current loss 0.157608  ||  Validation set: current loss 0.157744\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 274: Training set: current loss 0.157576  ||  Validation set: current loss 0.157718\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 275: Training set: current loss 0.157586  ||  Validation set: current loss 0.157720\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 276: Training set: current loss 0.157565  ||  Validation set: current loss 0.157712\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 277: Training set: current loss 0.157511  ||  Validation set: current loss 0.157703\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 278: Training set: current loss 0.157496  ||  Validation set: current loss 0.157710\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 279: Training set: current loss 0.157575  ||  Validation set: current loss 0.157672\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 280: Training set: current loss 0.157527  ||  Validation set: current loss 0.157672\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 281: Training set: current loss 0.157510  ||  Validation set: current loss 0.157655\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 282: Training set: current loss 0.157560  ||  Validation set: current loss 0.157623\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 283: Training set: current loss 0.157500  ||  Validation set: current loss 0.157623\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 284: Training set: current loss 0.157465  ||  Validation set: current loss 0.157600\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 285: Training set: current loss 0.157468  ||  Validation set: current loss 0.157583\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 286: Training set: current loss 0.157462  ||  Validation set: current loss 0.157604\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 287: Training set: current loss 0.157431  ||  Validation set: current loss 0.157573\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 288: Training set: current loss 0.157458  ||  Validation set: current loss 0.157573\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 289: Training set: current loss 0.157387  ||  Validation set: current loss 0.157563\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 290: Training set: current loss 0.157411  ||  Validation set: current loss 0.157553\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 291: Training set: current loss 0.157417  ||  Validation set: current loss 0.157540\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 292: Training set: current loss 0.157441  ||  Validation set: current loss 0.157526\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 293: Training set: current loss 0.157375  ||  Validation set: current loss 0.157494\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 294: Training set: current loss 0.157346  ||  Validation set: current loss 0.157501\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 295: Training set: current loss 0.157410  ||  Validation set: current loss 0.157487\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 296: Training set: current loss 0.157342  ||  Validation set: current loss 0.157478\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 297: Training set: current loss 0.157333  ||  Validation set: current loss 0.157459\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 298: Training set: current loss 0.157275  ||  Validation set: current loss 0.157456\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 299: Training set: current loss 0.157310  ||  Validation set: current loss 0.157451\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 300: Training set: current loss 0.157321  ||  Validation set: current loss 0.157406\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Begin reconstruction phase on test dataset:\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Autoencoder_RBM(rbm_layers=[1000, 500, 100],\n",
    "                              rbm_gauss_visible=True,\n",
    "                              finetune_num_epochs=300,\n",
    "                              do_pretrain=False,\n",
    "                              finetune_loss_func='mse')\n",
    "\n",
    "\n",
    "compres, recons, loss_summary = autoencoder.fit(np.array(df), validation_set=np.array(df), test_set = np.array(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAIUCAYAAAAHco0LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11db63b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_summary, 'g-', label='')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Reconstruction error MSE')\n",
    "plt.savefig('hd_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Training set: current loss 1.944235  ||  Validation set: current loss 1.944405\n",
      "Validation:  [2467 2496 2495 2466 2465 2468 2494 2464 2493 2603 2497 2492 2525 2602 2524\n",
      " 2601 2600 2523  906  907 2522 2526 2463 2604 2469]\n",
      "Iter 2: Training set: current loss 1.688586  ||  Validation set: current loss 1.688321\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2497 2525 2492 2602 2524\n",
      " 2601 2523 2600  907  906 2522 2463 2526 2604 2469]\n",
      "Iter 3: Training set: current loss 1.464708  ||  Validation set: current loss 1.464400\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2497 2492 2602 2524\n",
      " 2601 2523 2600  907  906 2522 2463 2526 2604 2469]\n",
      "Iter 4: Training set: current loss 1.272146  ||  Validation set: current loss 1.272123\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2497 2492 2602 2524\n",
      " 2523 2601 2600  907  906 2522 2463 2526 2469 2604]\n",
      "Iter 5: Training set: current loss 1.107331  ||  Validation set: current loss 1.107549\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2497 2602 2524\n",
      " 2523 2601 2600  907  906 2522 2463 2526  908 2469]\n",
      "Iter 6: Training set: current loss 0.969695  ||  Validation set: current loss 0.969677\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2497 2602 2524\n",
      " 2523 2601 2600  907 2522  906 2463 2526  908 2469]\n",
      "Iter 7: Training set: current loss 0.852260  ||  Validation set: current loss 0.852482\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2497 2602 2524\n",
      " 2523 2601 2600  907 2522  906 2463 2526  908 2469]\n",
      "Iter 8: Training set: current loss 0.755433  ||  Validation set: current loss 0.755583\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2497\n",
      " 2523 2601 2600  907 2522  906 2463 2526  908 2469]\n",
      "Iter 9: Training set: current loss 0.675049  ||  Validation set: current loss 0.675105\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2497\n",
      " 2523 2601 2600  907 2522  906 2463 2526  908 2469]\n",
      "Iter 10: Training set: current loss 0.606782  ||  Validation set: current loss 0.606746\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2497\n",
      " 2523 2601 2600  907 2522  906 2463 2526  908 2469]\n",
      "Iter 11: Training set: current loss 0.549635  ||  Validation set: current loss 0.549571\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2497 2600 2601  907 2522  906 2463 2526  908 2469]\n",
      "Iter 12: Training set: current loss 0.501980  ||  Validation set: current loss 0.502021\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2497 2600 2601  907 2522  906 2463 2526  908 2469]\n",
      "Iter 13: Training set: current loss 0.462142  ||  Validation set: current loss 0.462455\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2497 2600 2601  907 2522  906 2463 2526  908 2469]\n",
      "Iter 14: Training set: current loss 0.427809  ||  Validation set: current loss 0.428014\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2497 2600 2601  907 2522  906 2463 2526  908 2469]\n",
      "Iter 15: Training set: current loss 0.398992  ||  Validation set: current loss 0.399020\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2497 2600 2601  907 2522  906 2463 2526  908  909]\n",
      "Iter 16: Training set: current loss 0.374324  ||  Validation set: current loss 0.374412\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2497 2600 2601  907 2522  906 2463 2526  908  909]\n",
      "Iter 17: Training set: current loss 0.352949  ||  Validation set: current loss 0.353216\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2497 2601 2600  907 2522  906 2463 2526  908  909]\n",
      "Iter 18: Training set: current loss 0.334149  ||  Validation set: current loss 0.334276\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2497 2600  907 2522  906 2463 2526  908  909]\n",
      "Iter 19: Training set: current loss 0.316872  ||  Validation set: current loss 0.317064\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 20: Training set: current loss 0.301264  ||  Validation set: current loss 0.301472\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907 2522  906 2463  908 2526  909]\n",
      "Iter 21: Training set: current loss 0.286774  ||  Validation set: current loss 0.286935\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907 2522  906  908 2463 2526  909]\n",
      "Iter 22: Training set: current loss 0.273276  ||  Validation set: current loss 0.273545\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907 2522  906  908 2463 2526  909]\n",
      "Iter 23: Training set: current loss 0.260724  ||  Validation set: current loss 0.260884\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2492 2602 2524 2523\n",
      " 2601 2600 2497  907 2522  906  908 2463 2526  909]\n",
      "Iter 24: Training set: current loss 0.248653  ||  Validation set: current loss 0.249072\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2463 2526  909]\n",
      "Iter 25: Training set: current loss 0.237780  ||  Validation set: current loss 0.237800\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526 2463  909]\n",
      "Iter 26: Training set: current loss 0.226959  ||  Validation set: current loss 0.227025\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526 2463  909]\n",
      "Iter 27: Training set: current loss 0.216750  ||  Validation set: current loss 0.216866\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526 2463  909]\n",
      "Iter 28: Training set: current loss 0.207330  ||  Validation set: current loss 0.207509\n",
      "Validation:  [2467 2496 2495 2466 2465 2494 2468 2464 2493 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526 2463  909]\n",
      "Iter 29: Training set: current loss 0.198710  ||  Validation set: current loss 0.199010\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526  909 2463]\n",
      "Iter 30: Training set: current loss 0.191076  ||  Validation set: current loss 0.191527\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2492 2524 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526  909 2463]\n",
      "Iter 31: Training set: current loss 0.184393  ||  Validation set: current loss 0.184851\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526  909 2463]\n",
      "Iter 32: Training set: current loss 0.178847  ||  Validation set: current loss 0.179261\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908 2526  909 2463]\n",
      "Iter 33: Training set: current loss 0.174252  ||  Validation set: current loss 0.174578\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 34: Training set: current loss 0.170484  ||  Validation set: current loss 0.170567\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 35: Training set: current loss 0.167272  ||  Validation set: current loss 0.167547\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 36: Training set: current loss 0.164958  ||  Validation set: current loss 0.165190\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 37: Training set: current loss 0.163092  ||  Validation set: current loss 0.163311\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 38: Training set: current loss 0.161800  ||  Validation set: current loss 0.161988\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 39: Training set: current loss 0.160791  ||  Validation set: current loss 0.160950\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 40: Training set: current loss 0.160032  ||  Validation set: current loss 0.160258\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 41: Training set: current loss 0.159634  ||  Validation set: current loss 0.159775\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 42: Training set: current loss 0.159269  ||  Validation set: current loss 0.159389\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 43: Training set: current loss 0.159085  ||  Validation set: current loss 0.159184\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 44: Training set: current loss 0.158862  ||  Validation set: current loss 0.159019\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 45: Training set: current loss 0.158749  ||  Validation set: current loss 0.158911\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 46: Training set: current loss 0.158703  ||  Validation set: current loss 0.158812\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 47: Training set: current loss 0.158665  ||  Validation set: current loss 0.158728\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 48: Training set: current loss 0.158553  ||  Validation set: current loss 0.158710\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 49: Training set: current loss 0.158534  ||  Validation set: current loss 0.158682\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 50: Training set: current loss 0.158515  ||  Validation set: current loss 0.158661\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 51: Training set: current loss 0.158510  ||  Validation set: current loss 0.158628\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 52: Training set: current loss 0.158453  ||  Validation set: current loss 0.158622\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 53: Training set: current loss 0.158489  ||  Validation set: current loss 0.158602\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 54: Training set: current loss 0.158467  ||  Validation set: current loss 0.158599\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 55: Training set: current loss 0.158476  ||  Validation set: current loss 0.158591\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 56: Training set: current loss 0.158451  ||  Validation set: current loss 0.158608\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 57: Training set: current loss 0.158460  ||  Validation set: current loss 0.158587\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 58: Training set: current loss 0.158435  ||  Validation set: current loss 0.158568\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 59: Training set: current loss 0.158422  ||  Validation set: current loss 0.158554\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 60: Training set: current loss 0.158430  ||  Validation set: current loss 0.158580\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 61: Training set: current loss 0.158432  ||  Validation set: current loss 0.158561\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 62: Training set: current loss 0.158427  ||  Validation set: current loss 0.158568\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 63: Training set: current loss 0.158420  ||  Validation set: current loss 0.158565\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 64: Training set: current loss 0.158425  ||  Validation set: current loss 0.158551\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 65: Training set: current loss 0.158347  ||  Validation set: current loss 0.158549\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 66: Training set: current loss 0.158399  ||  Validation set: current loss 0.158546\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 67: Training set: current loss 0.158435  ||  Validation set: current loss 0.158528\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 68: Training set: current loss 0.158419  ||  Validation set: current loss 0.158533\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 69: Training set: current loss 0.158395  ||  Validation set: current loss 0.158524\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 70: Training set: current loss 0.158347  ||  Validation set: current loss 0.158526\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 71: Training set: current loss 0.158376  ||  Validation set: current loss 0.158538\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 72: Training set: current loss 0.158343  ||  Validation set: current loss 0.158531\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 73: Training set: current loss 0.158453  ||  Validation set: current loss 0.158510\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 74: Training set: current loss 0.158346  ||  Validation set: current loss 0.158518\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 75: Training set: current loss 0.158353  ||  Validation set: current loss 0.158541\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 76: Training set: current loss 0.158387  ||  Validation set: current loss 0.158520\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 77: Training set: current loss 0.158307  ||  Validation set: current loss 0.158519\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 78: Training set: current loss 0.158345  ||  Validation set: current loss 0.158509\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 79: Training set: current loss 0.158310  ||  Validation set: current loss 0.158520\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 80: Training set: current loss 0.158372  ||  Validation set: current loss 0.158516\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 81: Training set: current loss 0.158370  ||  Validation set: current loss 0.158502\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 82: Training set: current loss 0.158370  ||  Validation set: current loss 0.158489\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 83: Training set: current loss 0.158345  ||  Validation set: current loss 0.158479\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 84: Training set: current loss 0.158343  ||  Validation set: current loss 0.158481\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 85: Training set: current loss 0.158332  ||  Validation set: current loss 0.158492\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 86: Training set: current loss 0.158332  ||  Validation set: current loss 0.158476\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 87: Training set: current loss 0.158285  ||  Validation set: current loss 0.158466\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 88: Training set: current loss 0.158317  ||  Validation set: current loss 0.158476\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 89: Training set: current loss 0.158326  ||  Validation set: current loss 0.158464\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 90: Training set: current loss 0.158294  ||  Validation set: current loss 0.158469\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 91: Training set: current loss 0.158318  ||  Validation set: current loss 0.158457\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 92: Training set: current loss 0.158324  ||  Validation set: current loss 0.158472\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 93: Training set: current loss 0.158318  ||  Validation set: current loss 0.158457\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 94: Training set: current loss 0.158329  ||  Validation set: current loss 0.158449\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 95: Training set: current loss 0.158354  ||  Validation set: current loss 0.158432\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 96: Training set: current loss 0.158296  ||  Validation set: current loss 0.158434\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 97: Training set: current loss 0.158335  ||  Validation set: current loss 0.158420\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 98: Training set: current loss 0.158303  ||  Validation set: current loss 0.158436\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 99: Training set: current loss 0.158292  ||  Validation set: current loss 0.158443\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n",
      "Iter 100: Training set: current loss 0.158310  ||  Validation set: current loss 0.158428\n",
      "Validation:  [2467 2496 2495 2466 2494 2465 2468 2493 2464 2603 2525 2602 2524 2492 2523\n",
      " 2601 2600 2497  907  906 2522  908  909 2526 2463]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-016503763445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcompres_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecons_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_summary_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/sq/Documents/projects/projet3A/notebooks/autoencoder_rbm.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_set, validation_set, test_set, graph)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sq/Documents/projects/projet3A/notebooks/autoencoder_rbm.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, train_X, validation_X)\u001b[0m\n\u001b[1;32m    357\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                         self.keep_prob: self.finetune_dropout}\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Iter %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "autoencoder_2 = Autoencoder_RBM(rbm_layers=[2000, 500, 100],\n",
    "                              rbm_gauss_visible=True,\n",
    "                              finetune_num_epochs=300,\n",
    "                              do_pretrain=False,\n",
    "                              finetune_loss_func='mse')\n",
    "\n",
    "\n",
    "compres_2, recons_2, loss_summary_2 = autoencoder_2.fit(np.array(df), validation_set=np.array(df), test_set = np.array(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA08AAAIiCAYAAAAQI/MqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVoCTJzMzMzMzswL+H6p2e6o+x01NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11de5a9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def recons_error(input_set, recons):\n",
    "    dif = input_set - recons\n",
    "    dif = np.power(dif, 2)\n",
    "    return np.mean(dif, axis=1)\n",
    "\n",
    "reconstruction_error = recons_error(np.array(df), recons)\n",
    "plt.figure(figsize=(10, 6))\n",
    "min_bound = np.min(reconstruction_error) / 2.0\n",
    "max_bound = np.max(reconstruction_error)\n",
    "plot_range = max_bound - min_bound\n",
    "num_bins = 200\n",
    "bins = [ (min_bound + i * plot_range / num_bins) for i in range(num_bins+1)]\n",
    "freq, _, _ = plt.hist(reconstruction_error, bins=bins)\n",
    "\n",
    "index_abnormal_64 = [1081, 1082, 374, 375, 376, 377, 887, 888, 889, 2522, 2523, 2524, 2525, 2526, 2527]\n",
    "\n",
    "ax = plt.axes()\n",
    "for item in index_abnormal_64:\n",
    "    ax.axvline(reconstruction_error[item], color='r', linestyle='-')\n",
    "\n",
    "ax.set_xlim([min_bound, max_bound])\n",
    "plt.xlabel(r'reconstruction error')\n",
    "plt.ylabel(r'Frequence')\n",
    "plt.title('#compression layer units = 100')\n",
    "plt.savefig('hdp_1000_500_100.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
